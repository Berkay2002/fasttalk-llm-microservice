# ============================================================================
# FastTalk LLM Microservice - vLLM + PydanticAI Configuration
# ============================================================================
# Optimized for RTX 3090 Ti (24GB VRAM) with AWQ quantization
# Copy this file to .env and modify as needed.

# ============================================================================
# LLM Provider Selection
# ============================================================================
# Options: vllm, ollama, openai
LLM_PROVIDER=vllm

# ============================================================================
# vLLM Configuration (Primary Backend)
# ============================================================================
# vLLM runs the model LOCALLY on your GPU - no external API calls
VLLM_BASE_URL=http://vllm:8000/v1

# Model Selection - Using AWQ 4-bit quantized model for optimal 3090 Ti performance
# AWQ quantization: ~4.5GB VRAM for 8B model (vs 16GB for FP16)
VLLM_MODEL=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4

# Alternative quantized models (uncomment to use):
# VLLM_MODEL=TheBloke/Llama-2-13B-chat-AWQ          # 13B AWQ (~7GB VRAM)
# VLLM_MODEL=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4  # 70B AWQ (~40GB, needs multi-GPU)
# VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct      # Full precision (~16GB VRAM)

VLLM_API_KEY=not-needed
VLLM_TIMEOUT=600.0

# ============================================================================
# RTX 3090 Ti Optimized vLLM Server Settings
# ============================================================================
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_MAX_MODEL_LEN=8192
VLLM_GPU_MEMORY_UTILIZATION=0.90
VLLM_DTYPE=float16
VLLM_QUANTIZATION=awq
VLLM_PORT=8080
VLLM_SHM_SIZE=8g
VLLM_MEM_LIMIT=28g

# Performance tuning for RTX 3090 Ti
VLLM_MAX_NUM_SEQS=32
VLLM_MAX_NUM_BATCHED_TOKENS=8192
VLLM_SWAP_SPACE=4
VLLM_ENFORCE_EAGER=false

# HuggingFace token (for gated models like Llama)
# HF_TOKEN=your_huggingface_token_here

# ============================================================================
# PydanticAI Configuration
# ============================================================================
ENABLE_PYDANTIC_AI=true
ENABLE_WEB_SEARCH=true
ENABLE_TOOLS=true
DUCKDUCKGO_RATE_LIMIT=1.0

# System prompt for the voice agent
SYSTEM_PROMPT="You are a helpful voice assistant for FastTalk. Keep responses concise and conversational, suitable for speech synthesis. When asked about current events or facts you're unsure about, use web search."

# ============================================================================
# Ollama Configuration (Legacy/Fallback)
# ============================================================================
OLLAMA_BASE_URL=http://ollama:11434
LLM_MODEL=llama3.2:1b
OLLAMA_KEEP_ALIVE=5m
OLLAMA_TIMEOUT=600.0

# ============================================================================
# Generation Configuration
# ============================================================================
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=2048
DEFAULT_CONTEXT_WINDOW=8192
DEFAULT_TOP_P=0.9
DEFAULT_TOP_K=40

# ============================================================================
# Server Configuration
# ============================================================================
LLM_HOST=0.0.0.0
LLM_PORT=8000
LLM_MONITORING_PORT=9092
LLM_MAX_CONNECTIONS=50
LOG_LEVEL=INFO

# ============================================================================
# Session Configuration
# ============================================================================
SESSION_TIMEOUT=3600
MAX_HISTORY_LENGTH=50

# ============================================================================
# GPU Configuration
# ============================================================================
GPU_DEVICE_ID=0
CUDA_VISIBLE_DEVICES=0
NVIDIA_VISIBLE_DEVICES=0
COMPUTE_DEVICE=cuda

# ============================================================================
# Resource Limits
# ============================================================================
SHM_SIZE=2g
MEM_LIMIT=8g
LLM_SERVICE_MEM_LIMIT=8g

# ============================================================================
# Paths
# ============================================================================
MODEL_PATH=/app/models
LOG_PATH=/app/logs
LOGS_PATH=./logs
VLLM_CACHE_PATH=./vllm-cache

# ============================================================================
# Network
# ============================================================================
NETWORK_SUBNET=172.20.0.0/16
NETWORK_GATEWAY=172.20.0.1
